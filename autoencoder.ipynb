{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e8dec0-457d-4e9f-bdd1-a0eb028fcac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m8885/8885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.4281 - mae: 0.4139\n",
      "Epoch 2/5\n",
      "\u001b[1m8885/8885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 998us/step - loss: 0.3050 - mae: 0.3495 \n",
      "Epoch 3/5\n",
      "\u001b[1m8885/8885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.2847 - mae: 0.3301   \n",
      "Epoch 4/5\n",
      "\u001b[1m8885/8885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.2754 - mae: 0.3194\n",
      "Epoch 5/5\n",
      "\u001b[1m8885/8885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.2703 - mae: 0.3146\n",
      "\u001b[1m8901/8901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 672us/step\n",
      "[[270494  13821]\n",
      " [    72    420]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97    284315\n",
      "           1       0.03      0.85      0.06       492\n",
      "\n",
      "    accuracy                           0.95    284807\n",
      "   macro avg       0.51      0.90      0.52    284807\n",
      "weighted avg       1.00      0.95      0.97    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# (a) Import required libraries\n",
    "# ------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (b) Upload / access the dataset + preprocessing\n",
    "# ------------------------------------------------------------\n",
    "data = pd.read_csv('creditcard.csv')     # Load dataset\n",
    "\n",
    "# Normalize the \"Amount\" feature\n",
    "data['Amount'] = StandardScaler().fit_transform(data[['Amount']])\n",
    "\n",
    "# Drop the \"Time\" column (not needed)\n",
    "data = data.drop(['Time'], axis=1)\n",
    "\n",
    "# Use only normal transactions (Class = 0) for training\n",
    "# Autoencoder learns normal pattern, anomalies → high error\n",
    "x_train = data[data['Class'] == 0].drop(['Class'], axis=1).values\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (c) Encoder converts it into latent representation\n",
    "# ------------------------------------------------------------\n",
    "inp = Input((29,))                      # Input layer with 29 features\n",
    "enc = Dense(16, activation='relu')(inp)  # Encoder layer\n",
    "lat = Dense(8, activation='relu')(enc)   # Latent (compressed) representation\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (d) Decoder networks convert it back to original input\n",
    "# ------------------------------------------------------------\n",
    "dec = Dense(16, activation='relu')(lat)   # Decoder layer\n",
    "out = Dense(29, activation='linear')(dec) # Output layer reconstructing input\n",
    "\n",
    "# Build the complete Autoencoder model\n",
    "model = Model(inp, out)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (e) Compile the model with Optimizer, Loss, Evaluation Metrics\n",
    "# ------------------------------------------------------------\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "# Train the autoencoder\n",
    "# Training input = output, since goal is reconstruction\n",
    "model.fit(x_train, x_train, epochs=5, batch_size=32)\n",
    "\n",
    "\n",
    "X = data.drop([\"Class\"], axis=1).values\n",
    "y_true = data[\"Class\"].values\n",
    "\n",
    "recon = model.predict(X)\n",
    "mse = np.mean(np.square(X - recon), axis=1)\n",
    "threshold = np.percentile(mse, 95)\n",
    "y_pred = mse > threshold\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526975b-76cf-4adc-876a-0c551a9769d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
